{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is used to process SCOTUS cases and generate embedding as well as construct a graph from it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell connects to a Neo4j database using credentials stored in environment variables.\n",
    "The connection is verified by running a simple query to ensure the connection is successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Neo4j established successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read Neo4j credentials from environment variables\n",
    "neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "neo4j_password = os.getenv('NEO4J_PASSWORD')\n",
    "neo4j_uri = os.getenv('NEO4J_URI')\n",
    "\n",
    "# Create a Neo4j driver instance\n",
    "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Verify the connection\n",
    "\n",
    "def verify_connection(driver):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 1\")\n",
    "            if result.single()[0] == 1:\n",
    "                print(\"Connection to Neo4j established successfully.\")\n",
    "            else:\n",
    "                print(\"Failed to establish connection to Neo4j.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "verify_connection(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we connect to a MongoDB database using credentials stored in environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "MONGODB_USERNAME = os.getenv('MONGODB_USERNAME')\n",
    "MONGODB_PASSWORD = os.getenv('MONGODB_PASSWORD')\n",
    "MONGODB_HOST = os.getenv('MONGODB_HOST')\n",
    "MONGODB_DATABASE = os.getenv('MONGODB_DATABASE')\n",
    "\n",
    "mongo_uri = f\"mongodb+srv://{MONGODB_USERNAME}:{MONGODB_PASSWORD}@{MONGODB_HOST}/{MONGODB_DATABASE}?authSource=admin&replicaSet=db-mongo-graph-explorer\"\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[MONGODB_DATABASE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `recursive_text_splitter`:\n",
    "1. Takes a document and a maximum chunk size as input.\n",
    "2. It uses the `textwrap` module to split the document into lines of the specified maximum chunk size.\n",
    "3. If any chunk is still larger than the maximum chunk size, it recursively splits that chunk further.\n",
    "4. The function returns a list of final chunks, each of which is no larger than the specified maximum chunk size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def recursive_text_splitter(document, max_chunk_size):\n",
    "    # Use textwrap to initially split the text into lines of max_chunk_size\n",
    "    chunks = textwrap.wrap(document, width=max_chunk_size)\n",
    "    \n",
    "    final_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > max_chunk_size:\n",
    "            # If a chunk is still larger than max_chunk_size, split it further\n",
    "            final_chunks.extend(recursive_text_splitter(chunk, max_chunk_size))\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_embedding` function produces an embedding for a given `text`, using the `text-embedding-3-small` model from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell sets up the Pinecone client and initializes the index object that points to the Pinecone `socuts` index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40dcba58-adbd-4509-9323-f47d8cf22799\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Load Pinecone API key from environment variables\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "print(pinecone_api_key)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Create a Pinecone index\n",
    "index_name = \"scotus\"\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `save_opinion_embedding` function takes a `case_id` and `opinion_text` as input, and:\n",
    "1. Splits the `opinion_text` into smaller chunks using the `recursive_text_splitter` function.\n",
    "2. For each chunk, it generates an embedding using the `get_embedding` function.\n",
    "3. It then creates metadata for each chunk, including the `case_id` and the chunk itself.\n",
    "4. If any metadata value is `None`, it replaces it with the string \"null\".\n",
    "5. Finally, it upserts the chunk ID, embedding, and metadata into the Pinecone index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_embedding(case_id, opinion_text):\n",
    "    chunks = recursive_text_splitter(opinion_text, 512)\n",
    "    \n",
    "    # Create an index if it doesn't exist  \n",
    "    for chunk in chunks:\n",
    "        chunk_id = str(case_id) + \"_\" + str(chunks.index(chunk))\n",
    "        embedding = get_embedding(chunk)        \n",
    "        metadata = {\n",
    "            \"case_id\": str(case_id),\n",
    "            \"chunk\": chunk\n",
    "        }\n",
    "        \n",
    "        for key, value in metadata.items():\n",
    "            if value is None:\n",
    "                metadata[key] = \"null\" \n",
    "\n",
    "        index.upsert([(chunk_id, embedding, metadata)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below defines two classes, Node and Edge, which are used to create and manage nodes and edges in a Neo4j graph database.\n",
    "\n",
    "The Node class:\n",
    "- The constructor (__init__) initializes a node with an id, label, and properties. It escapes special characters in these values to ensure they are safe for Neo4j.\n",
    "- The `create_node_query` method generates a Cypher query to merge (create or update) a node with the given `id`, `label`, and `properties`.\n",
    "- The `create_node` method executes the query to create the node in the Neo4j database, but only if the `id`, `label`, and `properties` are valid (i.e., not None).\n",
    "\n",
    "The Edge class:\n",
    "- The constructor (__init__) initializes an edge with `from_node_id`, `to_node_id`, `relationship_type`, `properties`, and an `increment_property`. It escapes special characters in these values to ensure they are safe for Neo4j.\n",
    "- The `create_edge_query` method generates a Cypher query to merge (create or update) an edge with the given properties between the specified nodes. It also increments the value of the `increment_property` if the edge already exists.\n",
    "- The `create_edge` method executes the query to create the edge in the Neo4j database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def escape_neo4j_string(value):\n",
    "    if isinstance(value, str):\n",
    "        return re.sub(r\"(['\\\"\\\\])\", r\"\\\\\\1\", value)\n",
    "    return value\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id, label, properties):\n",
    "        self.id = escape_neo4j_string(id)\n",
    "        self.label = escape_neo4j_string(label)\n",
    "        self.properties = {k: escape_neo4j_string(v) if isinstance(v, str) else v for k, v in properties.items()}\n",
    "\n",
    "    def create_node_query(self):\n",
    "        props = ', '.join([f\"{key}: ${key}\" for key in self.properties.keys()])\n",
    "        return f\"MERGE (n:{self.label} {{id: '{self.id}', {props}}})\"\n",
    "\n",
    "    def create_node(self):\n",
    "        if not self.id or not self.label or any(value is None for value in self.properties.values()):\n",
    "            return\n",
    "        query = self.create_node_query()\n",
    "        with driver.session() as session:\n",
    "            session.run(query, **self.properties)\n",
    "\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, from_node_id, to_node_id, relationship_type, properties, increment_property):\n",
    "        self.from_node_id = escape_neo4j_string(from_node_id)\n",
    "        self.to_node_id = escape_neo4j_string(to_node_id)\n",
    "        self.relationship_type = escape_neo4j_string(relationship_type)\n",
    "        self.properties = {k: escape_neo4j_string(v) if isinstance(v, str) else v for k, v in properties.items()}\n",
    "        self.increment_property = increment_property\n",
    "\n",
    "    def create_edge_query(self):\n",
    "        props = ', '.join([f\"{key}: ${key}\" for key in self.properties.keys()])\n",
    "        return (f\"MATCH (a {{id: '{self.from_node_id}'}}), (b {{id: '{self.to_node_id}'}}) \"\n",
    "                f\"MERGE (a)-[r:{self.relationship_type} {{{props}}}]->(b) \"\n",
    "                f\"ON CREATE SET r.{self.increment_property} = 1 \"\n",
    "                f\"ON MATCH SET r.{self.increment_property} = r.{self.increment_property} + 1\")\n",
    "\n",
    "    def create_edge(self):\n",
    "        query = self.create_edge_query()\n",
    "        with driver.session() as session:\n",
    "            session.run(query, **self.properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we define the classes (or \"entities\" in our ontology based on the data structure found in the Oyez API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class Citation:\n",
    "    def __init__(self, data):\n",
    "        self.volume = data.get('volume')\n",
    "        self.page = data.get('page')\n",
    "        self.year = data.get('year')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.volume} U.S. {self.page} ({self.year})\"\n",
    "\n",
    "class Advocate:\n",
    "    def __init__(self, data):\n",
    "        advocate_data = data.get('advocate', {})\n",
    "        self.name = advocate_data.get('name') if advocate_data else None\n",
    "        self.description = data.get('advocate_description')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.description}\"\n",
    "\n",
    "class Decision:\n",
    "    def __init__(self, data):\n",
    "        self.description = data.get('description')\n",
    "        self.winning_party = data.get('winning_party')\n",
    "        self.decision_type = data.get('decision_type')\n",
    "        self.votes = [Vote(v) for v in data.get('votes', []) if v]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.description} - Winner: {self.winning_party}\"\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, data):\n",
    "        self.member = Justice(data.get('member', {}))\n",
    "        self.vote = data.get('vote')\n",
    "        self.opinion_type = data.get('opinion_type')\n",
    "        self.href = data.get('href')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.member.name}: {self.vote}\"\n",
    "\n",
    "class Justice:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('ID')\n",
    "        self.name = data.get('name')\n",
    "        self.roles = [Role(r) for r in data.get('roles', []) if r]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class Role:\n",
    "    def __init__(self, data):\n",
    "        self.type = data.get('type')\n",
    "        self.date_start = datetime.fromtimestamp(data.get('date_start', 0))\n",
    "        self.date_end = datetime.fromtimestamp(data.get('date_end', 0))\n",
    "        self.role_title = data.get('role_title')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.role_title} ({self.date_start.year}-{self.date_end.year})\"\n",
    "\n",
    "class DecidedBy:\n",
    "    def __init__(self, data):\n",
    "        self.name = data.get('name')\n",
    "        self.members = [Justice(j) for j in data.get('members', []) if j]\n",
    "\n",
    "class WrittenOpinion:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('id')\n",
    "        self.title = data.get('title')\n",
    "        self.author = data.get('author')\n",
    "        self.type_value = data.get('type', {}).get('value')\n",
    "        self.type_label = data.get('type', {}).get('label')\n",
    "        self.justia_opinion_id = data.get('justia_opinion_id')\n",
    "        self.justia_opinion_url = data.get('justia_opinion_url')\n",
    "        self.judge_full_name = data.get('judge_full_name')\n",
    "        self.judge_last_name = data.get('judge_last_name')\n",
    "        self.title_overwrite = data.get('title_overwrite')\n",
    "        self.href = data.get('href')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.title} ({self.type_label})\"\n",
    "\n",
    "\n",
    "\n",
    "class Case:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('ID')\n",
    "        self.name = data.get('name')\n",
    "        self.href = data.get('href')\n",
    "        self.docket_number = data.get('docket_number')\n",
    "        self.first_party = data.get('first_party')\n",
    "        self.first_party_label = data.get('first_party_label')\n",
    "        self.second_party = data.get('second_party')\n",
    "        self.second_party_label = data.get('second_party_label')\n",
    "        self.decided_date = datetime.fromtimestamp(data.get('timeline', [{}])[0].get('dates', [0])[0])\n",
    "        self.citation = Citation(data.get('citation', {}))\n",
    "        self.advocates = [Advocate(a) for a in data.get('advocates', []) if a] if data.get('advocates') else []\n",
    "        self.decisions = [Decision(d) for d in data.get('decisions', []) if d] if data.get('decisions') else []\n",
    "        self.decided_by = DecidedBy(data.get('decided_by', {})) if data.get('decided_by') else None\n",
    "        self.term = data.get('term')\n",
    "        self.justia_url = data.get('justia_url')\n",
    "        self.written_opinion = [WrittenOpinion(o) for o in data.get('written_opinion', []) if o] if data.get('written_opinion') else []\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name} ({self.term})\"\n",
    "\n",
    "    def print_details(self):\n",
    "        print(f\"Case: {self.name}\")\n",
    "        print(f\"Href: {self.href}\")\n",
    "        print(f\"Docket: {self.docket_number}\")\n",
    "        print(f\"Citation: {self.citation}\")\n",
    "        print(f\"Decided: {self.decided_date.strftime('%B %d, %Y')}\")\n",
    "        print(f\"Parties: {self.first_party} v. {self.second_party}\")\n",
    "        print(\"\\nAdvocates:\")\n",
    "        for advocate in self.advocates:\n",
    "            print(f\"  {advocate}\")\n",
    "        print(\"\\nDecisions:\")\n",
    "        for decision in self.decisions:\n",
    "            print(f\"  {decision}\")\n",
    "            for vote in decision.votes:\n",
    "                print(f\"    {vote}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports the `Sentence` and `Classifier` classes from the `flair` library, and:\n",
    "1. It loads two classifiers: one for named entity recognition (NER) and one for relation extraction. \n",
    "2. It defines two classes, Entity and Relation, to represent entities and relations extracted from text. \n",
    "3. The `extract_entities_and_relations` function takes a sentence as input, uses the NER classifier to identify entities, and the relation classifier to identify relationships between entities. \n",
    "4. Finally, it returns a list of entities and a list of relations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-23 13:44:47,416 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "# Load the NER and relation classifiers\n",
    "tagger = Classifier.load('ner')\n",
    "extractor = Classifier.load('relations')\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "class Relation:\n",
    "    def __init__(self, label, head, tail):\n",
    "        self.label = label\n",
    "        self.head = head\n",
    "        self.tail = tail\n",
    "\n",
    "\n",
    "def extract_entities_and_relations(sentence_text):\n",
    "    if not sentence_text:\n",
    "        return [], []\n",
    "    \n",
    "    sentence = Sentence(sentence_text)\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    tagger.predict(sentence)\n",
    "    for entity in sentence.get_labels('ner'):\n",
    "        entities.append(Entity(entity.data_point.text, entity.value))\n",
    "    \n",
    "    extractor.predict(sentence)\n",
    "    \n",
    "    relations = []\n",
    "\n",
    "    for relation in sentence.get_labels('relation'):\n",
    "        \n",
    "        head_text = relation.data_point.first.text\n",
    "        head_type = relation.data_point.first.get_label('ner').value\n",
    "        tail_text = relation.data_point.second.text\n",
    "        tail_type = relation.data_point.second.get_label('ner').value\n",
    "        \n",
    "        head_entity = Entity(head_text, head_type)\n",
    "        tail_entity = Entity(tail_text, tail_type)\n",
    "        relations.append(Relation(relation.value, head_entity, tail_entity))\n",
    "            \n",
    "    return entities, relations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, we have to helper functions:\n",
    "1. The `entity_to_node` function takes an `Entity` object as input and converts it into a `Node` object. The `Node` object is created with the entity's text as its ID, the entity's label as its label, and a dictionary containing the entity's text as its name property.\n",
    "\n",
    "2. The `relation_to_nodes_and_edges` function takes a `Relation` object as input and converts it into a list of two `Node` objects and an `Edge` object.\n",
    "   * It first converts the head and tail entities of the relation into `Node` objects using the `entity_to_node` function.\n",
    "   * Then, it creates an `Edge` object with the head node's ID, the tail node's ID, the relation's label, an empty dictionary for properties, and `count` as the edge type.\n",
    "   * The function returns a list containing the two `Node` objects and the `Edge` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_to_node(entity: Entity):\n",
    "    return Node(entity.text, entity.label, {\"name\": entity.text})\n",
    "    \n",
    "def relation_to_nodes_and_edges(relation: Relation):\n",
    "    head_node = entity_to_node(relation.head)\n",
    "    tail_node = entity_to_node(relation.tail)      \n",
    "    relation_edge = Edge(head_node.id, tail_node.id, relation.label, {}, \"count\")    \n",
    "    return [[head_node, tail_node], relation_edge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This cell defines a function `process_scotus_opinion` that processes a Supreme Court opinion.\n",
    "1. It retrieves the opinion from a MongoDB collection using the opinion's ID.\n",
    "2. If the opinion is not found or does not contain content, it returns empty lists.\n",
    "3. The function then saves the opinion's embedding and splits the content into sentences.\n",
    "4. It initializes dictionaries to store unique entities and relations.\n",
    "5. For each sentence, it extracts entities and relations using the `extract_entities_and_relations` function.\n",
    "6. It calculates a hash for each entity and relation to ensure uniqueness and stores them in the dictionaries.\n",
    "7. Finally, it returns lists of unique entities and relations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Load environment variables\n",
    "opinions_collection = db['opinions']\n",
    "\n",
    "def calculate_hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "def process_scotus_opinion(writtenOpinion: WrittenOpinion, case_node: Node):\n",
    "    opinion = opinions_collection.find_one({'id': writtenOpinion.id})\n",
    "    if not opinion or \"content\" not in opinion:\n",
    "        return [], []\n",
    "    \n",
    "    save_opinion_embedding(case_node.id, opinion[\"content\"])\n",
    "    sentences = opinion['content'].split('. ')\n",
    "    \n",
    "    all_entities = {}\n",
    "    all_relations = {}\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        entities, relations = extract_entities_and_relations(sentence)\n",
    "        \n",
    "        for entity in entities:\n",
    "            if entity and entity.text:\n",
    "                entity_hash = calculate_hash(entity.text)\n",
    "                if entity_hash not in all_entities:\n",
    "                    all_entities[entity_hash] = entity\n",
    "        \n",
    "        for relation in relations:\n",
    "            if relation and relation.label:\n",
    "                relation_hash = calculate_hash(relation.label)\n",
    "                if relation_hash not in all_relations:\n",
    "                    all_relations[relation_hash] = relation\n",
    "    \n",
    "    return list(all_entities.values()), list(all_relations.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates through written opinions, processes each opinion to extract entities and relations, and then creates nodes and edges for these entities and relations in the graph database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scotus_opinions(written_opinions: list[WrittenOpinion], case_node: Node): \n",
    "  if written_opinions:\n",
    "    for opinion in written_opinions:\n",
    "      if opinion:\n",
    "        \n",
    "        opinion_node = Node(opinion.id, \"Opinion\", {\"title\": opinion.title, \"case_id\": case_node.id})\n",
    "        opinion_node.create_node()\n",
    "        case_opinion_edge = Edge(case_node.id, opinion_node.id, \"case_opinion\", {}, \"count\")\n",
    "        case_opinion_edge.create_edge()\n",
    "        \n",
    "        entities, relations = process_scotus_opinion(opinion, case_node)\n",
    "        if entities:\n",
    "          entities_nodes = [entity_to_node(entity) for entity in entities if entity]\n",
    "          for relation in relations:\n",
    "            if relation:\n",
    "              relation_nodes, relation_edge = relation_to_nodes_and_edges(relation)\n",
    "              head_node, tail_node = relation_nodes\n",
    "              if head_node and tail_node:\n",
    "                head_node.create_node()\n",
    "                tail_node.create_node()\n",
    "                relation_edge.create_edge()\n",
    "                # print(f\"head_node: {head_node}, tail_node: {tail_node}, relation_edge: {relation_edge}\")\n",
    "\n",
    "          for node in entities_nodes:\n",
    "            if node:\n",
    "              node.create_node()\n",
    "              mentioned_in_edge = Edge(case_node.id, node.id, \"mentioned_in\", {}, \"count\")\n",
    "              mentioned_in_edge.create_edge()\n",
    "\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function processes SCOTUS cases and their related entities (parties, advocates, justices, decisions, opinions) and creates corresponding nodes and edges in a graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_cases_collection = db.processed_cases\n",
    "\n",
    "def process_scotus_case(case: Case):\n",
    "  first_party = case.first_party\n",
    "  second_party = case.second_party\n",
    "  advocates = case.advocates if case.advocates else []\n",
    "  decisions = case.decisions if case.decisions else []\n",
    "  justices = case.decided_by.members if case.decided_by and case.decided_by.members else []\n",
    "  \n",
    "  case_node = Node(case.id, \"Case\", {\"name\": case.name, \"docket_number\": case.docket_number, \"term\": case.term, \"decided_date\": case.decided_date.strftime('%Y-%m-%d')})\n",
    "  case_node.create_node()\n",
    "  \n",
    "  if first_party:\n",
    "      first_party_node = Node(first_party,\"Party\", {\"name\": first_party})\n",
    "      first_party_node.create_node()\n",
    "      first_party_node_name = case.first_party_label      \n",
    "      case_party_edge_1 = Edge(case_node.id, first_party_node.id, first_party_node_name, {}, \"count\")\n",
    "      case_party_edge_1.create_edge()\n",
    "  \n",
    "  if second_party:\n",
    "      second_party_node = Node(second_party, \"Party\", {\"name\": second_party})\n",
    "      second_party_node.create_node()\n",
    "      second_party_node_name = case.second_party_label\n",
    "      case_party_edge_2 = Edge(case_node.id, second_party_node.id, second_party_node_name, {}, \"count\")\n",
    "      case_party_edge_2.create_edge()  \n",
    "  \n",
    "  for advocate in advocates:\n",
    "    advocate_node = Node(advocate.name, \"Advocate\", {\"name\": advocate.name, \"description\": advocate.description})\n",
    "    advocate_node.create_node()\n",
    "    advocate_edge = Edge(case_node.id, advocate_node.id, \"advocated_by\", {}, \"count\")\n",
    "    advocate_edge.create_edge()\n",
    "  \n",
    "  for justice in justices:\n",
    "    justice_node = Node(justice.id, \"Justice\", {\"name\": justice.name})\n",
    "    justice_node.create_node()\n",
    "    justice_edge = Edge(case_node.id, justice_node.id, \"decided_by\", {}, \"count\")\n",
    "    justice_edge.create_edge()\n",
    "    \n",
    "  for decision in decisions:\n",
    "    if decision.winning_party:\n",
    "        decision_node = Node(decision.winning_party, \"Party\", { \"name\": decision.winning_party})\n",
    "        decision_node.create_node()\n",
    "        decision_edge = Edge(case_node.id, decision_node.id, \"won_by\", {\n",
    "          \"decision_type\": decision.decision_type\n",
    "        }, \"count\")\n",
    "        decision_edge.create_edge()\n",
    "    for vote in decision.votes:\n",
    "      justice_node = Node(vote.member.id, \"Justice\", {\"name\": vote.member.name})\n",
    "      justice_node.create_node()\n",
    "      vote_edge = Edge(case_node.id, justice_node.id, vote.vote, {\n",
    "        \"opinion_type\": vote.opinion_type\n",
    "      }, \"count\")\n",
    "      vote_edge.create_edge()\n",
    "  \n",
    "\n",
    "  \n",
    "  process_scotus_opinions(case.written_opinion, case_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell is responsible for processing a sample of unprocessed cases from the MongoDB collection.\n",
    "\n",
    "1. The unprocessed_count variable counts the number of documents in the collection that are either not processed or do not have a `processed` field\n",
    "2. The skip_size variable is a random number between 0 and the difference between unprocessed_count and sample_size, ensuring that the sample is taken from different parts of the collection\n",
    "3. The sampled_cases variable retrieves a sample of unprocessed cases from the collection, skipping a random number of documents and limiting the result to the sample size.\n",
    "4. The for loop iterates over each case in the sampled_cases, processes it using the process_scotus_case function, and marks it as processed in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "processed_cases_collection = db.processed_cases\n",
    "sample_size = 50\n",
    "\n",
    "\n",
    "unprocessed_count = processed_cases_collection.count_documents({\"$or\": [{\"processed\": False}, {\"processed\": {\"$exists\": False}}]})\n",
    "skip_size = random.randint(0, max(0, unprocessed_count - sample_size))\n",
    "\n",
    "sampled_cases = processed_cases_collection.find({\"$or\": [{\"processed\": False}, {\"processed\": {\"$exists\": False}}]}).skip(skip_size).limit(sample_size)\n",
    "\n",
    "for case_data in tqdm(sampled_cases):\n",
    "    case = Case(case_data)\n",
    "    process_scotus_case(case)\n",
    "    \n",
    "    # Mark the case as processed\n",
    "    processed_cases_collection.update_one(\n",
    "        {\"_id\": case_data[\"_id\"]},\n",
    "        {\"$set\": {\"processed\": True}}\n",
    "    )\n",
    "\n",
    "# Close the MongoDB connection\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
