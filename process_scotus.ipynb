{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to Neo4j established successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Read Neo4j credentials from environment variables\n",
    "neo4j_username = os.getenv('NEO4J_USERNAME')\n",
    "neo4j_password = os.getenv('NEO4J_PASSWORD')\n",
    "neo4j_uri = os.getenv('NEO4J_URI')\n",
    "\n",
    "# Create a Neo4j driver instance\n",
    "driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_username, neo4j_password))\n",
    "\n",
    "# Verify the connection\n",
    "\n",
    "def verify_connection(driver):\n",
    "    try:\n",
    "        with driver.session() as session:\n",
    "            result = session.run(\"RETURN 1\")\n",
    "            if result.single()[0] == 1:\n",
    "                print(\"Connection to Neo4j established successfully.\")\n",
    "            else:\n",
    "                print(\"Failed to establish connection to Neo4j.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "verify_connection(driver)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def recursive_text_splitter(document, max_chunk_size):\n",
    "    # Use textwrap to initially split the text into lines of max_chunk_size\n",
    "    chunks = textwrap.wrap(document, width=max_chunk_size)\n",
    "    \n",
    "    final_chunks = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > max_chunk_size:\n",
    "            # If a chunk is still larger than max_chunk_size, split it further\n",
    "            final_chunks.extend(recursive_text_splitter(chunk, max_chunk_size))\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "    \n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40dcba58-adbd-4509-9323-f47d8cf22799\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Load Pinecone API key from environment variables\n",
    "pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "print(pinecone_api_key)\n",
    "\n",
    "# Initialize Pinecone client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# Create a Pinecone index\n",
    "index_name = \"scotus-improved\"\n",
    "pindex = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_opinion_embedding(case_id, opinion_text):\n",
    "    chunks = recursive_text_splitter(opinion_text, 512)\n",
    "    \n",
    "    # Create an index if it doesn't exist  \n",
    "    for chunk in chunks:\n",
    "        chunk_id = str(case_id) + \"_\" + str(chunks.index(chunk))\n",
    "        embedding = get_embedding(chunk)        \n",
    "        metadata = {\n",
    "            \"case_id\": str(case_id),\n",
    "            \"chunk\": chunk\n",
    "        }\n",
    "        \n",
    "        for key, value in metadata.items():\n",
    "            if value is None:\n",
    "                metadata[key] = \"null\" \n",
    "\n",
    "        pindex.upsert([(chunk_id, embedding, metadata)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def escape_neo4j_string(value):\n",
    "    if isinstance(value, str):\n",
    "        # Escape single quotes, double quotes, backslashes, and special characters\n",
    "        value = re.sub(r\"(['\\\"\\\\])\", r\"\\\\\\1\", value)\n",
    "        value = re.sub(r\"([{}:])\", r\"\\\\\\1\", value)\n",
    "        # Replace spaces with underscores in relationship types\n",
    "        value = re.sub(r\"\\s+\", \"_\", value)\n",
    "    return value\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, id, label, properties):\n",
    "        self.id = escape_neo4j_string(id)\n",
    "        self.label = self.sanitize_label(label)\n",
    "        self.properties = {k: v if isinstance(v, str) else str(v) for k, v in properties.items()}\n",
    "\n",
    "    def sanitize_label(self, label):\n",
    "        # Remove any dots and replace with underscores\n",
    "        return escape_neo4j_string(str(label).replace('.', '_'))\n",
    "\n",
    "    def create_node_query(self):\n",
    "        props = ', '.join([f\"{key}: ${key}\" for key in self.properties.keys()])\n",
    "        if 'name' in self.properties:\n",
    "            return f\"\"\"\n",
    "            MERGE (n:{self.label} {{id: '{self.id}'}})\n",
    "            ON CREATE SET n += $properties\n",
    "            WITH n\n",
    "            MATCH (existing:{self.label} {{name: $name}})\n",
    "            WHERE existing.id <> n.id\n",
    "            WITH n, existing\n",
    "            SET n += properties(existing)\n",
    "            RETURN n\n",
    "            \"\"\"\n",
    "        else:\n",
    "            return f\"MERGE (n:{self.label} {{id: '{self.id}', {props}}})\"\n",
    "\n",
    "    def create_node(self):\n",
    "        if not self.id or not self.label or any(value is None for value in self.properties.values()):\n",
    "            return\n",
    "        query = self.create_node_query()\n",
    "        escaped_props = {k: v.replace(\"'\", \"\\\\'\") if isinstance(v, str) else v for k, v in self.properties.items()}\n",
    "        with driver.session() as session:\n",
    "            session.run(query, properties=escaped_props, **escaped_props)\n",
    "\n",
    "\n",
    "class Edge:\n",
    "    def __init__(self, from_node_id, to_node_id, relationship_type, properties, increment_property):\n",
    "        self.from_node_id = escape_neo4j_string(from_node_id)\n",
    "        self.to_node_id = escape_neo4j_string(to_node_id)\n",
    "        self.relationship_type = escape_neo4j_string(relationship_type)\n",
    "        self.properties = {k: escape_neo4j_string(v) if isinstance(v, str) else v for k, v in properties.items()}\n",
    "        self.increment_property = increment_property\n",
    "\n",
    "    def create_edge_query(self):\n",
    "        props = ', '.join([f\"{key}: ${key}\" for key in self.properties.keys()])\n",
    "        return (f\"MATCH (a {{id: '{self.from_node_id}'}}), (b {{id: '{self.to_node_id}'}}) \"\n",
    "                f\"MERGE (a)-[r:{self.relationship_type} {{{props}}}]->(b) \"\n",
    "                f\"ON CREATE SET r.{self.increment_property} = 1 \"\n",
    "                f\"ON MATCH SET r.{self.increment_property} = r.{self.increment_property} + 1\")\n",
    "\n",
    "    def create_edge(self):\n",
    "        query = self.create_edge_query()\n",
    "        with driver.session() as session:\n",
    "            session.run(query, **self.properties)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "\n",
    "class Relation:\n",
    "    def __init__(self, label, head, tail):\n",
    "        self.label = label\n",
    "        self.head = head\n",
    "        self.tail = tail\n",
    "\n",
    "class Citation:\n",
    "    def __init__(self, data):\n",
    "        self.volume = data.get('volume')\n",
    "        self.page = data.get('page')\n",
    "        self.year = data.get('year')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.volume} U.S. {self.page} ({self.year})\"\n",
    "\n",
    "class Advocate:\n",
    "    def __init__(self, data):\n",
    "        advocate_data = data.get('advocate', {})\n",
    "        self.name = advocate_data.get('name') if advocate_data else None\n",
    "        self.description = data.get('advocate_description')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name}: {self.description}\"\n",
    "\n",
    "class Decision:\n",
    "    def __init__(self, data):\n",
    "        self.description = data.get('description')\n",
    "        self.winning_party = data.get('winning_party')\n",
    "        self.decision_type = data.get('decision_type')\n",
    "        self.votes = [Vote(v) for v in data.get('votes', []) if v]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.description} - Winner: {self.winning_party}\"\n",
    "\n",
    "class Vote:\n",
    "    def __init__(self, data):\n",
    "        self.member = Justice(data.get('member', {}))\n",
    "        self.vote = data.get('vote')\n",
    "        self.opinion_type = data.get('opinion_type')\n",
    "        self.href = data.get('href')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.member.name}: {self.vote}\"\n",
    "\n",
    "class Justice:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('ID')\n",
    "        self.name = data.get('name')\n",
    "        self.roles = [Role(r) for r in data.get('roles', []) if r]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class Role:\n",
    "    def __init__(self, data):\n",
    "        self.type = data.get('type')\n",
    "        self.date_start = datetime.fromtimestamp(data.get('date_start', 0))\n",
    "        self.date_end = datetime.fromtimestamp(data.get('date_end', 0))\n",
    "        self.role_title = data.get('role_title')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.role_title} ({self.date_start.year}-{self.date_end.year})\"\n",
    "\n",
    "class DecidedBy:\n",
    "    def __init__(self, data):\n",
    "        self.name = data.get('name')\n",
    "        self.members = [Justice(j) for j in data.get('members', []) if j]\n",
    "\n",
    "class WrittenOpinion:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('id')\n",
    "        self.title = data.get('title')\n",
    "        self.author = data.get('author')\n",
    "        self.type_value = data.get('type', {}).get('value')\n",
    "        self.type_label = data.get('type', {}).get('label')\n",
    "        self.justia_opinion_id = data.get('justia_opinion_id')\n",
    "        self.justia_opinion_url = data.get('justia_opinion_url')\n",
    "        self.judge_full_name = data.get('judge_full_name')\n",
    "        self.judge_last_name = data.get('judge_last_name')\n",
    "        self.title_overwrite = data.get('title_overwrite')\n",
    "        self.href = data.get('href')\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.title} ({self.type_label})\"\n",
    "\n",
    "\n",
    "\n",
    "class Case:\n",
    "    def __init__(self, data):\n",
    "        self.id = data.get('ID')\n",
    "        self.name = data.get('name')\n",
    "        self.href = data.get('href')\n",
    "        self.docket_number = data.get('docket_number')\n",
    "        self.first_party = data.get('first_party')\n",
    "        self.first_party_label = data.get('first_party_label')\n",
    "        self.second_party = data.get('second_party')\n",
    "        self.second_party_label = data.get('second_party_label')\n",
    "        self.decided_date = datetime.fromtimestamp(data.get('timeline', [{}])[0].get('dates', [0])[0])\n",
    "        self.citation = Citation(data.get('citation', {}))\n",
    "        self.advocates = [Advocate(a) for a in data.get('advocates', []) if a] if data.get('advocates') else []\n",
    "        self.decisions = [Decision(d) for d in data.get('decisions', []) if d] if data.get('decisions') else []\n",
    "        self.decided_by = DecidedBy(data.get('decided_by', {})) if data.get('decided_by') else None\n",
    "        self.term = data.get('term')\n",
    "        self.justia_url = data.get('justia_url')\n",
    "        self.written_opinion = [WrittenOpinion(o) for o in data.get('written_opinion', []) if o] if data.get('written_opinion') else []\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.name} ({self.term})\"\n",
    "\n",
    "    def print_details(self):\n",
    "        print(f\"Case: {self.name}\")\n",
    "        print(f\"Href: {self.href}\")\n",
    "        print(f\"Docket: {self.docket_number}\")\n",
    "        print(f\"Citation: {self.citation}\")\n",
    "        print(f\"Decided: {self.decided_date.strftime('%B %d, %Y')}\")\n",
    "        print(f\"Parties: {self.first_party} v. {self.second_party}\")\n",
    "        print(\"\\nAdvocates:\")\n",
    "        for advocate in self.advocates:\n",
    "            print(f\"  {advocate}\")\n",
    "        print(\"\\nDecisions:\")\n",
    "        for decision in self.decisions:\n",
    "            print(f\"  {decision}\")\n",
    "            for vote in decision.votes:\n",
    "                print(f\"    {vote}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-19 14:57:26,599 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.nn import Classifier\n",
    "\n",
    "# Load the NER and relation classifiers\n",
    "tagger = Classifier.load('ner')\n",
    "extractor = Classifier.load('relations')\n",
    "\n",
    "class Entity:\n",
    "    def __init__(self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Entity(text='{self.text}', label='{self.label}')\"\n",
    "\n",
    "class Relation:\n",
    "    def __init__(self, label, head, tail):\n",
    "        self.label = label\n",
    "        self.head = head\n",
    "        self.tail = tail\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Relation(label='{self.label}', head={self.head}, tail={self.tail})\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_entities_and_relations(sentence_text):\n",
    "    if not sentence_text:\n",
    "        return [], []\n",
    "    \n",
    "    sentence = Sentence(sentence_text)\n",
    "    \n",
    "    entities = []\n",
    "\n",
    "    tagger.predict(sentence)\n",
    "    for entity in sentence.get_labels('ner'):\n",
    "        entities.append(Entity(entity.data_point.text, entity.value))\n",
    "    \n",
    "    extractor.predict(sentence)\n",
    "    \n",
    "    relations = []\n",
    "\n",
    "\n",
    "# Process relations (edges)\n",
    "    for relation in sentence.get_labels('relation'):\n",
    "        \n",
    "        head_text = relation.data_point.first.text\n",
    "        head_type = relation.data_point.first.get_label('ner').value\n",
    "        tail_text = relation.data_point.second.text\n",
    "        tail_type = relation.data_point.second.get_label('ner').value\n",
    "        \n",
    "        head_entity = Entity(head_text, head_type)\n",
    "        tail_entity = Entity(tail_text, tail_type)\n",
    "        relations.append(Relation(relation.value, head_entity, tail_entity))\n",
    "            \n",
    "    return entities, relations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel as BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import List, Union, Literal\n",
    "\n",
    "\n",
    "# Set up OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# # Define the model to use\n",
    "MODEL = \"gpt-4o-2024-08-06\"  # Make sure to use a model that supports Structured Outputs\n",
    "\n",
    "class LegalEntityType(Enum):\n",
    "    Case = \"Case\"   \n",
    "    Court = \"Court\"\n",
    "    Jurisdiction = \"Jurisdiction\"\n",
    "    Party = \"Party\"\n",
    "    Advocate = \"Advocate\"\n",
    "    Justice = \"Justice\"\n",
    "    Statute = \"Statute\"\n",
    "    Situation = \"Situation\"\n",
    "    Standard = \"Standard\"\n",
    "    Issue = \"Issue\"\n",
    "    Reasoning = \"Reasoning\"\n",
    "    PolicyArea = \"PolicyArea\"\n",
    "    Remedy = \"Remedy\"\n",
    "    Outcome = \"Outcome\"\n",
    "    Evidence = \"Evidence\"    \n",
    "    Motion = \"Motion\"        \n",
    "    \n",
    "class LegalRelationType(Enum):\n",
    "    follows_precedent = \"follows_precedent\"\n",
    "    distinguishes_from = \"distinguishes_from\"\n",
    "    overturns = \"overturns\"\n",
    "    interprets_statute = \"interprets_statute\"\n",
    "    applies_to = \"applies_to\"\n",
    "    construes = \"construes\"\n",
    "    asserts_jurisdiction_over = \"asserts_jurisdiction_over\"\n",
    "    claims_sovereign_immunity_from = \"claims_sovereign_immunity_from\"\n",
    "    determines_scope_of = \"determines_scope_of\"\n",
    "    applies_test = \"applies_test\"\n",
    "    consists_of = \"consists_of\"\n",
    "    establishes_standard_for = \"establishes_standard_for\"\n",
    "    dissents_on_grounds = \"dissents_on_grounds\"\n",
    "    concurs_with = \"concurs_with\"\n",
    "    criticizes = \"criticizes\"\n",
    "    impacts_policy_on = \"impacts_policy_on\"\n",
    "    considers_policy = \"considers_policy\"\n",
    "    affects = \"affects\"\n",
    "    orders_remedy = \"orders_remedy\"\n",
    "    results_in = \"results_in\"\n",
    "    provides_relief_to = \"provides_relief_to\"\n",
    "    argues_for = \"argues_for\"\n",
    "    challenges = \"challenges\"\n",
    "    seeks = \"seeks\"\n",
    "    interprets_constitutional_provision = \"interprets_constitutional_provision\"\n",
    "    violates = \"violates\"\n",
    "    upholds_constitutionality_of = \"upholds_constitutionality_of\"\n",
    "    admits = \"admits\"\n",
    "    supports = \"supports\"\n",
    "    rules_on = \"rules_on\"\n",
    "    preempts = \"preempts\"\n",
    "\n",
    "class LegalEntity(BaseModel):\n",
    "    type: LegalEntityType\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "class LegalTriple(BaseModel):\n",
    "    subject: LegalEntity\n",
    "    predicate: LegalRelationType\n",
    "    object: LegalEntity\n",
    "    text_reference: str\n",
    "    explanation: str\n",
    "\n",
    "class LegalAnalysis(BaseModel):\n",
    "    triples: List[LegalTriple]    \n",
    "\n",
    "LEGAL_ONTOLOGY = [\n",
    "    (\"Case\", \"establishes_principle\", \"Principle\"),\n",
    "    (\"Court\", \"reasons_that\", \"Argument\"),\n",
    "    (\"Decision\", \"based_on\", \"Reasoning\"),\n",
    "    (\"Case\", \"follows_precedent\", \"Case\"),\n",
    "    (\"Case\", \"distinguishes_from\", \"Case\"),\n",
    "    (\"Case\", \"overturns\", \"Case\"),\n",
    "    (\"Court\", \"interprets_statute\", \"Statute\"),\n",
    "    (\"Statute\", \"applies_to\", \"Situation\"),\n",
    "    (\"Court\", \"construes\", \"StatutoryLanguage\"),\n",
    "    (\"Court\", \"asserts_jurisdiction_over\", \"Matter\"),\n",
    "    (\"Entity\", \"claims_sovereign_immunity_from\", \"Jurisdiction\"),\n",
    "    (\"Court\", \"determines_scope_of\", \"Jurisdiction\"),\n",
    "    (\"Court\", \"applies_test\", \"Test\"),\n",
    "    (\"Test\", \"consists_of\", \"TestElement\"),\n",
    "    (\"Court\", \"establishes_standard_for\", \"Issue\"),\n",
    "    (\"Justice\", \"dissents_on_grounds\", \"Reasoning\"),\n",
    "    (\"Justice\", \"concurs_with\", \"MajorityOpinion\"),\n",
    "    (\"DissentingOpinion\", \"criticizes\", \"MajorityOpinion\"),\n",
    "    (\"Decision\", \"impacts_policy_on\", \"PolicyArea\"),\n",
    "    (\"Court\", \"considers_policy\", \"PolicyConsideration\"),\n",
    "    (\"Ruling\", \"affects\", \"SocialOrEconomicOutcome\"),\n",
    "    (\"Court\", \"orders_remedy\", \"Remedy\"),\n",
    "    (\"Decision\", \"results_in\", \"Outcome\"),\n",
    "    (\"Judgment\", \"provides_relief_to\", \"Party\"),\n",
    "    (\"Party\", \"argues_for\", \"Position\"),\n",
    "    (\"Party\", \"challenges\", \"Action\"),\n",
    "    (\"Plaintiff\", \"seeks\", \"Relief\"),\n",
    "    (\"Decision\", \"interprets_constitutional_provision\", \"ConstitutionalClause\"),\n",
    "    (\"Law\", \"violates\", \"ConstitutionalRight\"),\n",
    "    (\"Court\", \"upholds_constitutionality_of\", \"Statute\"),\n",
    "    (\"Court\", \"admits\", \"Evidence\"),\n",
    "    (\"Evidence\", \"supports\", \"Claim\"),\n",
    "    (\"Court\", \"rules_on\", \"ProceduralMotion\"),\n",
    "    (\"Party\", \"files\", \"Motion\"),\n",
    "    (\"Statute\", \"preempts\", \"StateLaw\")\n",
    "]\n",
    "\n",
    "def extract_legal_triples(text: str) -> List[LegalTriple]:\n",
    "    prompt = f\"\"\"\n",
    "    As an AI assistant specialized in legal analysis, your task is to analyze the given legal text and extract specific entities and their relationships based on the following ontology:\n",
    "\n",
    "    {LEGAL_ONTOLOGY}\n",
    "\n",
    "    For each relevant relationship found in the text:\n",
    "    1. Extract the specific entities that fit the subject and object types from the ontology.\n",
    "    2. Formulate a triple using these specific entities and the predicate from the ontology.\n",
    "    3. Include a brief quote or paraphrase from the text supporting the triple.\n",
    "    4. Offer a short explanation of the triple's significance in the context of the case.\n",
    "\n",
    "    Only include triples that are directly relevant to the given text. Do not force relationships that are not clearly present in the document.\n",
    "\n",
    "    Here is the legal text to analyze:\n",
    "\n",
    "    {text}\n",
    "\n",
    "    Remember to extract specific entities from the text, not just use generic terms. For each entity, provide its type (e.g., LegalCase, LegalCourt, etc.) and either a name or a description, depending on what's most appropriate for that entity type.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a legal analysis expert.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format=LegalAnalysis,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed.triples\n",
    "\n",
    "# def process_legal_document(document: str) -> List[LegalTriple]:\n",
    "#     legal_triples = extract_legal_triples(document)\n",
    "    \n",
    "#     entities = set()\n",
    "#     relations = []\n",
    "\n",
    "#     for triple in legal_triples:\n",
    "#         subject_entity = (triple.subject.type, triple.subject.name or triple.subject.description)\n",
    "#         object_entity = (triple.object.type, triple.object.name or triple.object.description)\n",
    "        \n",
    "#         entities.add(subject_entity)\n",
    "#         entities.add(object_entity)\n",
    "        \n",
    "#         relation = (subject_entity, triple.predicate, object_entity)\n",
    "#         relations.append(relation)\n",
    "    \n",
    "#     return list(entities), relations\n",
    "#     return extract_legal_triples(document)\n",
    "def process_legal_document(document: str) -> List[LegalTriple]:\n",
    "    return extract_legal_triples(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples extracted: 4\n",
      "\n",
      "Triple 1:\n",
      "Subject: LegalEntityType.Case - Oklahoma Tax Commission v. Citizen Band Potawatomi Indian Tribe of Oklahoma\n",
      "Predicate: LegalRelationType.follows_precedent\n",
      "Object: LegalEntityType.Case - United States v. United States Fidelity and Guaranty Co.\n",
      "Text Reference: \"The Court cited United States v. United States Fidelity and Guaranty Co. to support its reasoning on tribal sovereign immunity.\"\n",
      "Explanation: The Supreme Court relied on the precedent set by United States v. United States Fidelity and Guaranty Co. to reinforce its decision regarding tribal sovereign immunity, emphasizing the consistency in legal interpretations of tribal rights over time.\n",
      "\n",
      "Triple 2:\n",
      "Subject: LegalEntityType.Court - Supreme Court\n",
      "Predicate: LegalRelationType.asserts_jurisdiction_over\n",
      "Object: LegalEntityType.Jurisdiction - Tribal Sovereign Immunity\n",
      "Text Reference: \"The Supreme Court ruled in Oklahoma Tax Commission v. Citizen Band Potawatomi Indian Tribe of Oklahoma that under the doctrine of tribal sovereign immunity, Oklahoma may not tax sales to tribal members on tribal land.\"\n",
      "Explanation: The Court's ruling affirmed its authority to define the scope of tribal sovereign immunity, impacting the extent to which states can exercise power over tribal lands.\n",
      "\n",
      "Triple 3:\n",
      "Subject: LegalEntityType.Justice - Justice Stevens\n",
      "Predicate: LegalRelationType.concurs_with\n",
      "Object: LegalEntityType.Court - Supreme Court\n",
      "Text Reference: \"Justice Stevens concurred, noting the need for Congressional action to address state-tribal tax issues.\"\n",
      "Explanation: Justice Stevens agreed with the overall decision but highlighted the need for legislative measures, showing his support for the ruling while suggesting further governmental involvement.\n",
      "\n",
      "Triple 4:\n",
      "Subject: LegalEntityType.Court - Supreme Court\n",
      "Predicate: LegalRelationType.determines_scope_of\n",
      "Object: LegalEntityType.Jurisdiction - Tribal Reservation Status\n",
      "Text Reference: \"The Court also established that trust land qualifies as a reservation for tribal immunity purposes when it has been validly set apart for the use of Indians under government superintendence.\"\n",
      "Explanation: The determination clarifies the legal status of trust lands under federal and state law, affecting the administration of such lands as tribal reservations.\n"
     ]
    }
   ],
   "source": [
    "def process_legal_document(document: str) -> List[LegalTriple]:\n",
    "    return extract_legal_triples(document)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"\"\"\n",
    "    The Supreme Court ruled in Oklahoma Tax Commission v. Citizen Band Potawatomi Indian Tribe of Oklahoma that under the doctrine of tribal sovereign immunity, \n",
    "    Oklahoma may not tax sales to tribal members on tribal land, but the tribe has an obligation to assist in collecting valid state taxes on \n",
    "    sales to non-members. The Court cited United States v. United States Fidelity and Guaranty Co. to support its reasoning on tribal sovereign immunity. \n",
    "    Justice Stevens concurred, noting the need for Congressional action to address state-tribal tax issues. The Court also established that trust land \n",
    "    qualifies as a reservation for tribal immunity purposes when it has been validly set apart for the use of Indians under government superintendence.\n",
    "    \"\"\"\n",
    "    \n",
    "    legal_triples = process_legal_document(sample_text)\n",
    "    print(f\"Total triples extracted: {len(legal_triples)}\")\n",
    "    for i, triple in enumerate(legal_triples, 1):\n",
    "        print(f\"\\nTriple {i}:\")\n",
    "        print(f\"Subject: {triple.subject.type} - {triple.subject.name or triple.subject.description}\")\n",
    "        print(f\"Predicate: {triple.predicate}\")\n",
    "        print(f\"Object: {triple.object.type} - {triple.object.name or triple.object.description}\")\n",
    "        print(f\"Text Reference: \\\"{triple.text_reference}\\\"\")\n",
    "        print(f\"Explanation: {triple.explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import hashlib\n",
    "def calculate_hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legal_triples_to_entities_and_relations(triples: List[LegalTriple]):\n",
    "    entities = {}\n",
    "    relations = []\n",
    "\n",
    "    for triple in triples:\n",
    "        subject_entity = Entity(triple.subject.name if hasattr(triple.subject, 'name') else str(triple.subject), triple.subject.type.value)\n",
    "        object_entity = Entity(triple.object.name if hasattr(triple.object, 'name') else str(triple.object), triple.object.type.value)\n",
    "\n",
    "        # Use hash to avoid duplicate entities\n",
    "        subject_hash = calculate_hash(subject_entity.text)\n",
    "        object_hash = calculate_hash(object_entity.text)\n",
    "\n",
    "        if subject_hash not in entities:\n",
    "            entities[subject_hash] = subject_entity\n",
    "        if object_hash not in entities:\n",
    "            entities[object_hash] = object_entity\n",
    "\n",
    "        relation = Relation(triple.predicate.value, subject_entity, object_entity)\n",
    "        relations.append(relation)\n",
    "\n",
    "    return list(entities.values()), relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_to_node(entity: Entity):\n",
    "    return Node(entity.text, entity.label, {\"name\": entity.text})\n",
    "    \n",
    "\n",
    "def relation_to_nodes_and_edges(relation: Relation):\n",
    "    head_node = entity_to_node(relation.head)\n",
    "    tail_node = entity_to_node(relation.tail)      \n",
    "    relation_edge = Edge(head_node.id, tail_node.id, relation.label, {}, \"count\")    \n",
    "    return [[head_node, tail_node], relation_edge]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinions_db = TinyDB('opinions.db.json')\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def calculate_hash(text):\n",
    "    return hashlib.md5(text.encode()).hexdigest()\n",
    "\n",
    "def process_scotus_opinion(writtenOpinion: WrittenOpinion, case_node: Node):\n",
    "    opinion = opinions_db.get(where('id') == writtenOpinion.id)\n",
    "    if not opinion or \"content\" not in opinion:\n",
    "        return [], []\n",
    "    \n",
    "    legal_triples = process_legal_document(opinion[\"content\"])\n",
    "    entities, relations = legal_triples_to_entities_and_relations(legal_triples)\n",
    "    save_opinion_embedding(case_node.id, opinion[\"content\"])\n",
    "    return entities, relations\n",
    "        \n",
    "    \n",
    "    # sentences = opinion['content'].split('. ')\n",
    "    \n",
    "    # all_entities = {}\n",
    "    # all_relations = {}\n",
    "    \n",
    "    # for sentence in sentences:\n",
    "    #     entities, relations = extract_entities_and_relations(sentence)\n",
    "        \n",
    "    #     for entity in entities:\n",
    "    #         if entity and entity.text:\n",
    "    #             entity_hash = calculate_hash(entity.text)\n",
    "    #             if entity_hash not in all_entities:\n",
    "    #                 all_entities[entity_hash] = entity\n",
    "        \n",
    "    #     for relation in relations:\n",
    "    #         if relation and relation.label:\n",
    "    #             relation_hash = calculate_hash(relation.label)\n",
    "    #             if relation_hash not in all_relations:\n",
    "    #                 all_relations[relation_hash] = relation\n",
    "    \n",
    "    # return list(all_entities.values()), list(all_relations.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scotus_opinions(written_opinions: list[WrittenOpinion], case_node: Node): \n",
    "  if written_opinions:\n",
    "    for opinion in written_opinions:\n",
    "      if opinion:\n",
    "        \n",
    "        opinion_node = Node(opinion.id, \"Opinion\", {\"title\": opinion.title, \"case_id\": case_node.id})\n",
    "        opinion_node.create_node()\n",
    "        case_opinion_edge = Edge(case_node.id, opinion_node.id, \"case_opinion\", {}, \"count\")\n",
    "        case_opinion_edge.create_edge()\n",
    "        \n",
    "        entities, relations = process_scotus_opinion(opinion, case_node)\n",
    "        if entities:\n",
    "          entities_nodes = [entity_to_node(entity) for entity in entities if entity]\n",
    "          for relation in relations:\n",
    "            if relation:\n",
    "              relation_nodes, relation_edge = relation_to_nodes_and_edges(relation)\n",
    "              head_node, tail_node = relation_nodes\n",
    "              if head_node and tail_node:\n",
    "                head_node.create_node()\n",
    "                tail_node.create_node()\n",
    "                relation_edge.create_edge()\n",
    "\n",
    "          for node in entities_nodes:\n",
    "            if node:\n",
    "              node.create_node()\n",
    "              mentioned_in_edge = Edge(case_node.id, node.id, \"mentioned_in\", {}, \"count\")\n",
    "              mentioned_in_edge.create_edge()\n",
    "\n",
    "        print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scotus_case(case: Case):\n",
    "  first_party = case.first_party\n",
    "  second_party = case.second_party\n",
    "  advocates = case.advocates if case.advocates else []\n",
    "  decisions = case.decisions if case.decisions else []\n",
    "  justices = case.decided_by.members if case.decided_by and case.decided_by.members else []\n",
    "  \n",
    "  case_node = Node(case.id, \"Case\", {\"name\": case.name, \"docket_number\": case.docket_number, \"term\": case.term, \"decided_date\": case.decided_date.strftime('%Y-%m-%d')})\n",
    "  case_node.create_node()\n",
    "  \n",
    "  if first_party:\n",
    "      first_party_node = Node(first_party,\"Party\", {\"name\": first_party})\n",
    "      first_party_node.create_node()\n",
    "      first_party_node_name = case.first_party_label      \n",
    "      case_party_edge_1 = Edge(case_node.id, first_party_node.id, first_party_node_name, {}, \"count\")\n",
    "      case_party_edge_1.create_edge()\n",
    "  \n",
    "  if second_party:\n",
    "      second_party_node = Node(second_party, \"Party\", {\"name\": second_party})\n",
    "      second_party_node.create_node()\n",
    "      second_party_node_name = case.second_party_label\n",
    "      case_party_edge_2 = Edge(case_node.id, second_party_node.id, second_party_node_name, {}, \"count\")\n",
    "      case_party_edge_2.create_edge()  \n",
    "  \n",
    "  for advocate in advocates:\n",
    "    advocate_node = Node(advocate.name, \"Advocate\", {\"name\": advocate.name, \"description\": advocate.description})\n",
    "    advocate_node.create_node()\n",
    "    advocate_edge = Edge(case_node.id, advocate_node.id, \"advocated_by\", {}, \"count\")\n",
    "    advocate_edge.create_edge()\n",
    "  \n",
    "  for justice in justices:\n",
    "    justice_node = Node(justice.id, \"Justice\", {\"name\": justice.name})\n",
    "    justice_node.create_node()\n",
    "    justice_edge = Edge(case_node.id, justice_node.id, \"decided_by\", {}, \"count\")\n",
    "    justice_edge.create_edge()\n",
    "    \n",
    "  for decision in decisions:\n",
    "    if decision.winning_party:\n",
    "        decision_node = Node(decision.winning_party, \"Party\", { \"name\": decision.winning_party})\n",
    "        decision_node.create_node()\n",
    "        decision_edge = Edge(case_node.id, decision_node.id, \"won_by\", {\n",
    "          \"decision_type\": decision.decision_type\n",
    "        }, \"count\")\n",
    "        decision_edge.create_edge()\n",
    "    for vote in decision.votes:\n",
    "      justice_node = Node(vote.member.id, \"Justice\", {\"name\": vote.member.name})\n",
    "      justice_node.create_node()\n",
    "      vote_edge = Edge(case_node.id, justice_node.id, vote.vote, {\n",
    "        \"opinion_type\": vote.opinion_type\n",
    "      }, \"count\")\n",
    "      vote_edge.create_edge()\n",
    "  \n",
    "  # print(len(case.written_opinion))  \n",
    "  process_scotus_opinions(case.written_opinion, case_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987c147c729447c49b79a052d313fcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................................................................................................................................................................................................."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m sampled_cases \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(expanded_cases, \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m case_data \u001b[38;5;129;01min\u001b[39;00m tqdm(sampled_cases):\n\u001b[0;32m---> 16\u001b[0m     case \u001b[38;5;241m=\u001b[39m \u001b[43mCase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# case.print_details()\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# print(\"\\n\")\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     process_scotus_case(case)\n",
      "Cell \u001b[0;32mIn[9], line 108\u001b[0m, in \u001b[0;36mCase.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcitation \u001b[38;5;241m=\u001b[39m Citation(data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvocates \u001b[38;5;241m=\u001b[39m [Advocate(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvocates\u001b[39m\u001b[38;5;124m'\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m a] \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvocates\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecisions \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mDecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdecisions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecisions\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecided_by \u001b[38;5;241m=\u001b[39m DecidedBy(data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecided_by\u001b[39m\u001b[38;5;124m'\u001b[39m, {})) \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecided_by\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterm \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 108\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcitation \u001b[38;5;241m=\u001b[39m Citation(data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcitation\u001b[39m\u001b[38;5;124m'\u001b[39m, {}))\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvocates \u001b[38;5;241m=\u001b[39m [Advocate(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvocates\u001b[39m\u001b[38;5;124m'\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m a] \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madvocates\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecisions \u001b[38;5;241m=\u001b[39m [\u001b[43mDecision\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecisions\u001b[39m\u001b[38;5;124m'\u001b[39m, []) \u001b[38;5;28;01mif\u001b[39;00m d] \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecisions\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecided_by \u001b[38;5;241m=\u001b[39m DecidedBy(data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecided_by\u001b[39m\u001b[38;5;124m'\u001b[39m, {})) \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecided_by\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterm \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mDecision.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwinning_party \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwinning_party\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_type \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecision_type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvotes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mVote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvotes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "from ipywidgets import FloatProgress\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "db = TinyDB('cases.db.json')\n",
    "\n",
    "expanded_cases = db.all()\n",
    "\n",
    "\n",
    "sampled_cases = random.sample(expanded_cases, 100)\n",
    "\n",
    "\n",
    "\n",
    "for case_data in tqdm(sampled_cases):\n",
    "    case = Case(case_data)\n",
    "    # case.print_details()\n",
    "    \n",
    "\n",
    "    # print(\"\\n\")\n",
    "    process_scotus_case(case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "env_vars_to_delete = [\n",
    "    \"OPENAI_API_KEY\",\n",
    "    \"PINECONE_API_KEY\",\n",
    "    \"NEO4J_URI\",\n",
    "    \"NEO4J_USERNAME\",\n",
    "    \"NEO4J_PASSWORD\"\n",
    "]\n",
    "\n",
    "\n",
    "for var in env_vars_to_delete:\n",
    "    if var in os.environ:\n",
    "        del os.environ[var]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
